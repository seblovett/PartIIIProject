%% ----------------------------------------------------------------
%% InvestigationVision.tex
%% ---------------------------------------------------------------- 
\chapter{Investigation into Vision Algorithms} \label{Chapter:InvestigationVision}

\section{Matching Algorithms}\label{Section:Comparison}
%\inote{find some references to back these claims up}
%\inote{Talk about how to compare images and TEST them all. Make a final comparison to decide on which will be used}
In computer vision, there are many different ways of comparing two similar images. These include the sum of absolute differences (S.A.D.) (\cite{Hamzah:DistanceDetection}), the sum of squared differences (S.S.D.)(\cite{Mrovlje:Distance_Stereoscopic}) and  normalised cross correlation (N.C.C.)(\cite{zhao2006image}). Each of these methods will be explained and tested to compare them. All testing will use images seen in figure \ref{fig:StereoTest}. Each test uses the same size window ($50\times50$) to compare the two images. 

\begin{figure}
\centering
\subfigure[Left Image]{\includegraphics[scale=0.4]{./Figures/deskLeft.png} }
\subfigure[Right Image]{\includegraphics[scale=0.4]{./Figures/deskRight.png} }
\caption{Stereoscopic Test Images from MATLAB Examples}
\label{fig:StereoTest}
\end{figure}


%Explanation of how they work
%\inote{Maybe do a basic 5x5 example for each?}
\subsection{Sum of Absolute Differences}\label{Section:SAD}

Given two identically sized two dimensional matrices, $A, B$, of dimensions $I,J$, SAD is defined as
\begin{equation} \label{eq:SAD}
SAD = \sum\limits_{i=0}^{I-1} \sum\limits_{j=0}^{J-1} A[i,j] - B[i,j] 
\end{equation}

This method subtracts the observed window from the expected. All differences are then added together. This algorithm is simple and requires a small amount of computation. The algorithm returns values where a small result means the two images are well matched.

\subsection{Sum of Squared Differences}\label{Section:SSD}
\begin{equation}\label{eq:SSD}
SSD = \sum\limits_{i=0}^{I-1} \sum\limits_{j=0}^{J-1} (A[i,j] - B[i,j] )^2
\end{equation}

This is very similar to S.A.D. but adds more complexity by squaring each difference. This removes the ability of equally different but opposite differences cancelling each other out (grey to white of one pixel will cancel out a white to grey difference in the other with SAD). Again, a low result is a match in this case.

%\inote{sort chi out, if I want to do it...}
%\subsection{'Chi Squared'}
%$\chi ^{2}$ is ``Insert definition here". For use with images the equation can be adapted to \ref{eq:ChiSquare}. 
%
%\begin{equation} \label{eq:ChiSquare}
%\chi ^{2} = \sum\limits_{i=0}^{I-1} \sum\limits_{j=0}^{J-1}\frac{(A[i,j] - B[i,j])^2}{(A[i,j]+B[i,j])/2}
%\end{equation}

\inote{test effect of box size?}
\subsection{NCC}\label{Section:NCC}
\begin{equation}\label{eq:NCC}
NCC =  \frac{1}{n}\sum\limits_{i,j} \frac{(A[i,j] - \bar{A}).(B[i,j] - \bar{B})}{\sigma _A . \sigma _B}
\end{equation}
\begin{center}
Where $n$ is the number of pixels in $A$ and $B$, \\$\sigma$ is the standard deviation of the image, and \\$\bar{A}$ is the a average pixel value. 
%\inote{Find a source for this equation}
\end{center}
%\inote{No date on Reference}
NCC is very similar to cross correlation, but normalised to reduce the error if one image is brighter than the other. This is common in computer vision (\cite{Tsai:NCC}) and cross correlation is a often used in digital signal processing, so fast algorithms have been made to calculate this. 

Unlike S.S.D. and S.A.D., the normalised cross correlation gives a high value for a match. The downside to this algorithm comes with the complexity of the equation as it contains division and the a square root of a number in order to calculate the standard deviation. These operations are rarely implemented in hardware and are time consuming to carry out in software. They also require floating point registers and operates slowly on a microcontroller without any. 



%test and compare
\subsection{Comparison}

To compare these equations, a 50 by 50 window taken from the right picture was compared with the left image over the entire valid range. The coordinates on the graph give the centre pixel of the calculation. 

\begin{figure}
\centering
\subfigure[S.A.D Results (Low match)\label{fg:Results:SAD}]{\includegraphics[width = 12cm, keepaspectratio]{./Figures/SADResults.jpg} }
\subfigure[S.S.D. Results (Low match)\label{fg:Results:SSD}]{\includegraphics[width = 12cm, keepaspectratio]{./Figures/SSDResults.jpg} }
\subfigure[N.C.C. Results (High match)\label{fg:Results:NCC}]{\includegraphics[width = 12cm, keepaspectratio]{./Figures/NCCResults.jpg} }
\caption{Result Graphs of Comparison Algorithms}
\label{fg:CompResults}
\end{figure}

Each graph shows the correct area being identified as a match, but this also highlights the downfalls of the SAD and SSD. The figures in figure \ref{fg:CompResults} are rotated to match the orientation of the images in figure \ref{fig:StereoTest}. Each of the images is tested by attempting to match the desk phone from the right image to the entirety of the left image. The actual match should be around $(170, 176)$. An exact result cannot be estimated as the images are not matched perfectly - there isn't an exact integer of pixel difference between the images. This is the sub pixel problem (\cite{haller2012design}).

SAD results in figure \ref{fg:Results:SSD} show large areas of matching. A minimum occurs around the location expected($170,175$) of a value of $5.66\times 10^4$. However, along the bottom of the image, where a dark area occurs below the desk in the lower part of figures \ref{fig:StereoTest}, the SAD algorithm detects a greater comparison, with the lowest value in this area being $3370$ at $(227, 275)$. This creates a false detection here. 

SSD shows matches in the same two areas: where a match should occur and the dark area beneath the desk. The minimum value where the match should occur is $4.355 \times 10^5$ at location $(170,176)$. However, there is a large match correlation between the dark area under the desk where the actual lowest value of $2.768\times10^4$ occurs at $(225,274)$. This, again, is a false match and is a downfall of this algorithm. 

The NCC results are visible in figure \ref{fg:Results:NCC}. A match can be seen at coordinate $(195,201)$ with a peak value of $0.9654$. The coordinate is different to the previous results because the cross correlation works over the boundary of the image creating more results. The dimensions of the image are 300 $\times$ 400, but the NCC returns an data set of dimensions 350 $\times$ 450 when using a window size of $50\times 50$. To get the actual match, half of the box size must be subtracted from the returned coordinate. This means the match occurs at $(170,176)$. With this algorithm, there is no area of the image which is close to a false detection. 

\subsection{Conclusion}
It can be seen that there is a direct correlation between the complexity of the matching algorithm to the reliability of the match returned. In brightly lit, colourful environments absent of dark colours, SAD and SSD should provide a reliable result, but this cannot be guaranteed to always be the case. Therefore further development of the matching algorithm will start with using the normalised cross correlation. A comprise between complexity and reliability needs to be reached, where reliability is the more desirable of the two. Cross correlation is also a large area of research, so optimised algorithms do exist.

\section{Range Finding}
\subsection{Derivations}

By using two images separated by a horizontal distance, $B$, the range of an object can be found given some characteristics of the camera. The following are derivations of the equations used to calculate distance. 

The problem is broken down into three parts:
\begin{enumerate}
\item Object is between the cameras (Figure \ref{problem_between})
\item Object is in left or right hand sides of both images (Figure \ref{problem_toleft})
\item Object is directly in front of a camera (Figure \ref{fig:problem_infront})
\end{enumerate}

\begin{figure}
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/problem1.png}
\caption{Problem 1 - Object is between the Cameras}
\label{problem_between}
\end{figure}

\subsubsection{Object is between the Cameras}
Derivation from \cite{Mrovlje:Distance_Stereoscopic}.
\begin{equation} \label{eq:B}
B = B_{1} + B_{2} = D\tan(\varphi_{1}) + D\tan(\varphi_{2})
\end{equation}

\begin{equation} \label{eq:D}
D = \frac{B}{\tan(\varphi_{1}) + \tan(\varphi_{2})}
\end{equation}


%\begin{figure}
%\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/left_simplified.png}
%\caption{Problem 1 : Left Camera Simplified}
%\label{Left_Simplified}
%\end{figure}

\begin{equation} \label{eq:phi}
D\tan\left(\frac{\varphi_{0}}{2}\right) = \frac{x_{0}}{2}
\end{equation}

\begin{equation} \label{eq:phi1}
D\tan(\varphi_1) = x_1
\end{equation}

Dividing \eqref{eq:phi1} by \eqref{eq:phi}

\begin{equation} \label{eq:tanovertan}
\frac{\tan(\varphi_1)}{\tan(\frac{\varphi_0}{2})} = \frac{2x_1}{x_0}
\end{equation}

\begin{equation} \label{eq:phionesolved}
\tan(\varphi_1) = \frac{2x_1\tan(\frac{\varphi_0}{2})}{x_0}
\end{equation}

This can also be shown for the right camera:

\begin{equation} \label{eq:phitwosolved}
\tan(\varphi_2) = \frac{-2x_2\tan(\frac{\varphi_0}{2})}{x_0}
\end{equation}

Substitution equations \eqref{eq:phionesolved} and \eqref{eq:phitwosolved} into \eqref{eq:D} gives

\begin{equation} \label{eq:Distance1}
D = \frac{Bx_0}{2\tan(\frac{\varphi_0}{2})(x_1 - x_2)}
\end{equation}


\subsubsection{Object is to the same side in each camera}
Derivation is based on the derivation from \cite{DistanceEstimation}. Using figure \ref{problem_toleft}:

\begin{equation} \label{eq:p2:tanphi1}
D.\tan(\varphi_{1}) = x_{1}
\end{equation}

\begin{equation} \label{eq:p2:tanphi0}
D.\tan\left(\frac{\varphi_{0}}{2}\right) = \frac{x_{0}}{2}
\end{equation}

\begin{equation} \label{eq:p2:tanphi0and1}
\frac{\tan(\varphi_{1})}{\tan(\frac{\varphi_0}{2})} = \frac{2x_1}{x_0}
\end{equation}

\begin{equation} \label{eq:p2:phi1}
\varphi_1 = \arctan\left(\frac{2x_1}{x_0}\tan\left(\frac{\varphi_0}{2}\right)\right)
\end{equation}

and similarly
\begin{equation} \label{eq:p2:phi2}
\varphi_2 = \arctan\left(\frac{2x_2}{x_0}\tan\left(\frac{\varphi_0}{2}\right)\right)
\end{equation}
\begin{equation} \label{eq:p2:theta}
\theta = \varphi_2 - \varphi_1
\end{equation}

Using the sine equality rule:

\begin{equation} \label{eq:p2:sineeq}
\frac{R}{\sin(\frac{\pi}{2} - \varphi_2)} = \frac{B}{\sin(\theta)}
\end{equation}

\begin{equation} \label{eq:p2:R}
R = B.\frac{\sin(\frac{\pi}{2} - \varphi_2)}{\sin(\theta)} = B \frac{\cos(\varphi_2)}{\sin(\theta)}
\end{equation}

\begin{equation} \label{eq:p2:DeqR}
D = \cos(\varphi_1).R
\end{equation}
Substituting \eqref{eq:p2:theta} into \eqref{eq:p2:R}, and then into \eqref{eq:p2:DeqR}:

\begin{equation} \label{eq:p2:DeqB}
D = B.\frac{\cos(\varphi_2).\cos(\varphi_1)}{sin(\varphi_2 - \varphi_1)}
\end{equation}

Where $\varphi_1$ is defined in equation \eqref{eq:p2:phi1} and $\varphi_2$ is defined in equation \eqref{eq:p2:phi2}. 

\begin{figure}
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/problem2.png}
\caption{Problem 2 - Object is to the same side in both cameras}
\label{problem_toleft}
\end{figure}

\subsubsection{Object is in front of a camera}
The distance, $D$, in this problem is given by:

\begin{equation} \label{eq:p3:D}
D = B \tan\left(\frac{\pi}{2} - \varphi_{2}\right)
\end{equation}
Where $\varphi_2$ can be found from equation \ref{eq:p2:phi2}. 
\begin{figure}
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{Figures/problem3.png}
\caption{Problem 3 - Object is directly in front of a camera}
\label{fig:problem_infront}
\end{figure}

\subsection{Summary}
There are three situations that can occur. These are listed below with their equations.

Object is between the two cameras:
\begin{equation} \label{eq:summary:1}
D = \frac{Bx_0}{2\tan(\frac{\varphi_0}{2})(x_1 - x_2)}
\end{equation}

Object is to the same side in both images:
\begin{equation} \label{eq:summary:2}
D = B.\frac{\cos(\varphi_2).\cos(\varphi_1)}{sin(\varphi_2 - \varphi_1)}
\end{equation}
Object is directly in front of a camera:
\begin{equation} \label{eq:summary:3}
D = B \tan\left(\frac{\pi}{2} - \varphi_{2}\right)
\end{equation}

Where $\varphi_1$ is defined in equation \eqref{eq:p2:phi1} and $\varphi_2$ is defined in equation \eqref{eq:p2:phi2}.

When the images have been matched, these equations can be used to calculate the range to an object.


\section{Fourier Transform}
\subsection{Background Research and the FFT}
The Fourier Transform is a common tool in signal processing. It transforms a time based signal to the frequency domain showing the frequency components contained in the signal as a complex number, which is often displayed as magnitude and phase. The Fourier Transform is defined in equation \eqref{eq:fourier} and two examples of signals and their Fourier Transforms are shown in figures \ref{fig:DiracFunctionFT} and \ref{fig:SquareWaveFT}. 
 
\begin{equation}\label{eq:fourier}
X(f) = \int\limits_{-\infty}^{\infty}x(t)e^{-\jmath 2 \pi ft}dt
\end{equation}

\begin{figure}
\centering
\subfigure[A graph showing a Dirac Function\label{fg:Dirac:Signal}]{\includegraphics[width=\textwidth, keepaspectratio]{./Figures/Dirac_1D_Sig.jpg} }
\subfigure[A graph showing the magnitude of the Fourier transform of the Dirac Function\label{fg:Dirac:Mag}]{\includegraphics[width=\textwidth, keepaspectratio]{./Figures/Dirac_1D_Mag.jpg} }
\subfigure[A graph showing the phase of the Fourier transform of the Dirac Function\label{fg:Dirac:Phase}]{\includegraphics[width=\textwidth, keepaspectratio]{./Figures/Dirac_1D_Phase.jpg} }
\caption{A Dirac signal and the phase and magnitude of its Fourier Transform}
\label{fig:DiracFunctionFT}
\end{figure}

\begin{figure}
\centering
\subfigure[A graph showing rectangular pulse\label{fg:Square:Signal}]{\includegraphics[width=\textwidth, keepaspectratio]{./Figures/Square_1D_Sig.jpg} }
\subfigure[A graph showing the magnitude of the Fourier transform of the rectangular pulse\label{fg:Square:Mag}]{\includegraphics[width=\textwidth, keepaspectratio]{./Figures/Square_1D_Mag.jpg} }
\subfigure[A graph showing the phase of the Fourier transform of the rectangular pulse\label{fg:Square:Phase}]{\includegraphics[width=\textwidth, keepaspectratio]{./Figures/Square_1D_Phase.jpg} }
\caption{A 2D Rectangular pulse and the phase and magnitude of its Fourier Transform}
\label{fig:SquareWaveFT}
\end{figure}

\inote{Uses of FT in signal processing}
%\inote{Definition of DFT and theory of FFT}
The equation for the Fourier transform in equation \eqref{eq:fourier} is for continuous time. A discrete Fourier transform (DFT) exists for finite, equally spaced samples. This is commonly used in digital systems and is defined in equation \eqref{eq:DFT}. There exists a Fast Fourier Transform (FFT) which gives exactly the same results as the DFT, but is optimised in terms of number of multiplications done. The FFT will be used in implementation due to availability of code and speed of use. 

\begin{equation}\label{eq:DFT}
X[k] = \sum\limits_{0}^{N-1}x[n]e^{-\jmath \Omega_0 kn}
\end{equation}
Where $\Omega_0$ is the sample frequency
\inote{Why this is relevant to my project - what am I using it for?}
A property of the Fourier Transform of interest is the convolution theorem which states that convolution in time is multiplication in frequency and is defined mathematically in equation \eqref{eq:ConvolutionMultiplication}. As discussed in section \ref{Section:NCC}, cross correlation is very similar to convolution. Convolution is defined in equation \eqref{eq:CrossCorrelation}. With images, $f(t)$ is a real signal, its conjugate is exactly the same, $f(t) \equiv f^*(t) \text{given that} f(t) \in \Re$. 


\begin{equation}\label{eq:ConvolutionMultiplication}
\int\limits_{-\infty}^{\infty}f(\tau)g(t-\tau)d\tau = f(t) \ast g(t) = X(f)\cdot Y(f)
\end{equation}
\begin{equation}\label{eq:CrossCorrelation}
\int\limits_{-\infty}^{\infty}f^*(\tau)g(t+\tau)d\tau = f(t) \star g(t) = X(f)\cdot Y(f)
\end{equation}

\begin{equation}\label{eq:CCtoConv}
f(t) \star g(t) = f(t) \ast g(-t) = F(f) \cdot G(-f)
\end{equation}

\inote{example of convolution used as correlation. Include some pretty graphs}
\subsection{Two Dimensional Fast Fourier Transform}
\inote{Definition}
A two dimensional Fourier transform exists for analysing two dimensional signals, namely in this application, an image. The Fourier Transform is shown in equation \eqref{eq:2dFT} and the discrete version is shown in \eqref{eq:2dDFT}

\begin{equation}\label{eq:2dFT}
F(u,v) = \frac{1}{2\pi}\int\limits_{-\infty}^{\infty}\int\limits_{-\infty}^{\infty}f(x,y)e^{-2\pi\jmath (xu+yv)}dxdy
\end{equation}
\begin{equation}\label{eq:2dDFT}
F(u,v) = \frac{1}{N} \sum\limits_{x=0}^{N-1}\sum\limits_{y=0}^{N-1}f(x,y)e^{-\frac{2\pi\jmath (xu+yv)}{N}} \; \; \; \; \; x,y,u,v \in \left\lbrace 0\dots N-1\right\rbrace
\end{equation}
\inote{Examples}
Figures \ref{fig:2DDiracFunctionFT} and \ref{fig:2DSquareWaveFT} show the two dimensional equivalent test signals of figures \ref{fg:Dirac:Signal} and \ref{fg:Square:Signal} and the phase and magnitudes of their Fourier Transforms. There is a direct similarity between the 1D and 2D spectra; the magnitudes of the Dirac (figures \ref{fg:Dirac:Mag} and \ref{fg:Dirac2D:Mag}) are both constant values and the rectangular pulses both have a modulus sinc function magnitude (figures \ref{fg:Square:Signal} and \ref{fg:Square2D:Signal}). 

\begin{figure}
\centering
\subfigure[An image of a 2D Dirac Function\label{fg:Dirac2D:Signal}]{\includegraphics[width=10cm, keepaspectratio]{./Figures/Dirac_2D_Sig.jpg} }
\subfigure[An image of the magnitude of the Fourier transform of the 2D Dirac Function\label{fg:Dirac2D:Mag}]{\fbox{\includegraphics[width=(\textwidth / 2)-1cm, keepaspectratio]{./Figures/Dirac_2D_Mag.jpg} }}
\subfigure[An image of the phase of the Fourier transform of the 2D Dirac Function\label{fg:Dirac2D:Phase}]{\includegraphics[width =(\textwidth / 2)-1cm, keepaspectratio]{./Figures/Dirac_2D_Phase.jpg} }
\caption{A 2D Dirac signal and the phase and magnitude of its Fourier Transform}
\label{fig:2DDiracFunctionFT}
\end{figure}

\begin{figure}
\centering
\subfigure[An image of the 2D rectangular pulse\label{fg:Square2D:Signal}]{\includegraphics[width =10cm, keepaspectratio]{./Figures/Square_2D_Sig.jpg} }
\subfigure[An image of the magnitude of the Fourier transform of the 2D rectangular pulse\label{fg:Square2D:Mag}]{\includegraphics[width =(\textwidth / 2)-1cm, keepaspectratio]{./Figures/Square_2D_Abs.jpg} }
\subfigure[An image of the phase of the Fourier transform of the 2D rectangular pulse\label{fg:Square2D:Phase}]{\includegraphics[width =(\textwidth / 2)-1cm, keepaspectratio]{./Figures/Square_2D_Phase.jpg} }
\caption{A 2D Rectangular Pulse signal and the phase and magnitude of its Fourier transform}
\label{fig:2DSquareWaveFT}
\end{figure}


\inote{2D FFT and restrictions}
The 2D Fourier transform can also be optimised to a fast Fourier transform algorithm in a similar way as the 1D case. The algorithm, sometimes referred to as the Butterfly transform, is briefly discussed in \cite{nixon2012feature} where is is explained that the algorithm can be easily applied to images with equal dimensions that are a power of 2. The algorithm utilises the separability property of the Fourier transform. 

The 2D FFT can be implemented using a 1D FFT as follows:
\begin{enumerate}
\item Calculate the 1D FFT of each of the rows of the 2D data. (An FFT of data of length $n$ returns an array, also of length $n$)
\item Calculate the 1D FFT of each of the columns of the 2D data returned from the previous step.
\end{enumerate}
Total number of FFTs done is $2n$ where $n$ is the height/width of the image. 
\inote{Maybe make a figure to help explain?}
\subsection{Implementing the FFT}
\inote{Include and explain code}
The Atmel Software Framework \citep{Atmel:ASF} included a digital signal processing library. This contained functions to compute the FFT of a real or complex array, the inverse FFT, magnitude and phase of complex data. Further restrictions are imposed by the DSP library used as the data must be an even power of 2, and that the data is in fixed point notation. This gives a usable dimension of $256 \times 256$ for processing images on the AVR. Though the height of an image from the OV7670 camera is $240$, the image can be transformed so that it repeats for 16 rows at the bottom as the Fourier transform works on an assumption of the data repeating itself.

The function \textit{FFT2DCOMPLEX} in Appendix \ref{Appendix:Code:ImageProcessor.c} is the realisation of a two dimensional fast Fourier transform on the microcontroller. The FFT function requires the data to be 4 byte aligned (A\_ALIGNED) and of type \textit{dsp16\_complex\_t}. The data must be given in fixed point notation and it is returned in fixed point notation. A 16 bit representation was chosen over 32 bit due to being more functions for 16 bit data available. 


\subsection{Testing of the FFT on AVR}
\inote{Show results of some test signals}
\subsubsection{1D FFT Test}
A Dirac function and a rectangular pulse were used as test signals. 

Figure \ref{fig:AVR:FFT:Dirac:Input} shows the input signal given to the AVR. It is a 256 long array of a Dirac function. This was then converted to the internally defined fixed point notation and passed through the Fourier transform method. The resulting complex array was then saved to a Comma Separated Value file and read into MATLAB. Figure \ref{fig:AVR:FFT:Dirac:Output} shows the calculated phase and magnitude plots of the output complex array. The magnitude is relatively flat and around the value of 1. In comparison with figure \ref{fg:Dirac:Mag}, they are relatively similar. The phase, however, seems to be very different. Figure \ref{fg:Dirac:Phase} shows what was expected, but the two phase results seem not to match. This could due to MATLAB having more accurate algorithms and a more accurate representation that the 16 bit fixed point used on the AVR. However, using a function in the DSP library to calculate the magnitude, the spectrum in figure \ref{fig:AVR:FFT:Dirac:Mag} is obtained. This, though is not exactly 1 as expected, is completely flat. It is computed from the same transformed data. It suggests that there is some internal compensation in the algorithms. The actual value in figure \ref{fig:AVR:FFT:Dirac:Mag} is $0.9897$ to 4 decimal places giving an overall error of $1.03\%$. 

\begin{figure}
\includegraphics[width=\textwidth]{./Figures/AVR_FFT_Dirac_Input.jpg}
\caption{Input Dirac Signal for AVR fast Fourier transform}
\label{fig:AVR:FFT:Dirac:Input}
\end{figure}
\begin{figure}
\includegraphics[width=\textwidth]{./Figures/AVR_FFT_Dirac_Input.jpg}
\caption{Output phase and magnitude of the complex output from AVR fast Fourier transform of a Dirac function}
\label{fig:AVR:FFT:Dirac:Output}
\end{figure}

\begin{figure}
\includegraphics[width=\textwidth]{./Figures/AVR_FFT_Dirac_Input.jpg}
\caption{Maginitude calculated by the AVR of the Fourier transform of a Dirac function}
\label{fig:AVR:FFT:Dirac:Mag}
\end{figure}
\subsubsection{2D FFT Test}
\inote{Elapsed Time to do FT}
\inote{Need to implement grey scaling method before being able to do this}
\inote{Show results of actual photo being transformed}
\inote{Compare to MATLAB results}
\section{Low Level Vision Algorithms}
\subsection{Noise Reduction}
\inote{Why}
Noise exists in all signals. Two noise sources for the camera image is random noise in the sensor, and quantisation noise. It generates a compromise between noise and resolution that has to be made. Large amounts of noise reduction will blur edges and therefore reduce the quality of the image and make it harder to match. This section will investigate some noise reduction methods, and test if the application of them increases the reliability of matching using the Normalised Cross Correlation method discussed in section \ref{Section:NCC}. 
\inote{Theory}
\inote{Examples}
\inote{Does it improve the reliability of matching? Vary noise amount in images? and test}
\subsection{Edge Detection}
\inote{Why}
\inote{Theory}
\inote{Examples}
\inote{Does it improve the reliability of matching?}
